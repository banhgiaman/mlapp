import re
import os
from pyvi import ViTokenizer

def load_dataset(path):
    ds = []
    items = os.listdir(path)
    for item in items:
        item_path = '%s/%s' % (path, item)
        for file in os.listdir(item_path):
            file_path = '%s/%s' % (item_path, file)
            with open(file_path, 'r', encoding='utf-8') as f:
                ds.append(f.read())

    return ds

list = load_dataset('data_train')

sentiment_stopwords = ["ufeff", "+", "\"", "", ".", ",", "!", "%", "....", "...", ")", "(", "thÃ¬", "lÃ ", "vÃ ", "bá»‹", "vá»›i",
                       "tháº¿_nÃ o", "?", "", "má»™t_sá»‘", "mot_so", "thi", "la", "va", "bi", "voi", "trong",
                       "the_nao", " j ", "gÃ¬", "cÃ³", "pin", "giÃ¡", "j7pro", "chá»©", "mÃ¡y", "tÃ´i", "cá»§a", "Ä‘á»ƒ", "ai",
                       "sáº£n_pháº©m", "j7", "tháº¥y", "báº£n", "vÃ¬", "nÃªn", "ace", "pubg", "j5", "ip7", "ip7+", "nhÃ©", "nhe",
                       "nhÃ©'", "nhÆ°", "tá»« ", "váº­y", "2h", "thui", "thÃ´i", "bin`", "fb", "facebook", "youtube", "pr", "pháº£i"
                       "khi", "triá»‡u", "triá»‡u'", "18tr", "fan", "xÃ i", "láº¡i", "chá»¥p", "camera", "plus", "Ä‘iá»‡n_thoáº¡i",
                       "tá»›i", "web", "reset", "nguyÃªn_Ä‘Ã¡n", "s9", "j8", "mÃ n_hÃ¬nh", "64gb", "táº¿t", "nhÃ¢n_viÃªn"]

stopwords_dash = []
with open('stopwords-dash.txt', encoding='utf-8') as f:
    stopwords_dash = [t.replace("\n", "") for t in f.readlines()]


emotion_icons = {
    'ğŸ˜€': "positive", 'ğŸ˜¬': "positive", 'ğŸ˜': ' positive ', 'ğŸ˜‚': "positive ",
    'ğŸ˜ƒ': "positive", 'ğŸ˜„': "positive", 'ğŸ¤£': "positive", 'ğŸ˜…': "positive", 'ğŸ˜†': "positive", 'ğŸ˜‡': "positive",
    'ğŸ˜‰': "positive", 'ğŸ˜Š': "positive",  'ğŸ™‚': "positive", 'ğŸ™ƒ': "positive", 'â˜º': "positive", 'ğŸ˜‹':' positive ',
    'ğŸ˜Œ': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜—': ' positive ', 'ğŸ˜™': ' positive ',
    'ğŸ˜š': ' positive ', 'ğŸ¤ª': ' positive ', 'ğŸ˜œ': ' positive ', 'ğŸ˜': ' positive ',
    'ğŸ˜›': ' positive ', 'ğŸ¤‘': ' positive ', "ğŸ˜": "positive", "ğŸ¤“": "positive", 'ğŸ§': ' positive ', 'ğŸ¤ ': ' positive ',
    "ğŸ¤—": "positive", "ğŸ¤¡": "positive", "ğŸ˜": "negative", "ğŸ˜¶": "negative", "ğŸ˜": "negative", "ğŸ˜‘": "negative",
    "ğŸ˜’": "negative", "ğŸ™„": "negative", "ğŸ¤¨": "negative", "ğŸ¤”": "negative", "ğŸ¤«": "negative", 'ğŸ¤­': ' positive ',
    'ğŸ¤¥': ' negative ', 'ğŸ˜³': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜ ': ' negative ',
    'ğŸ˜¡': ' negative ', 'ğŸ¤¬': ' negative ', 'ğŸ˜”': ' negative ', 'ğŸ˜•': ' negative ',
    'ğŸ™': ' negative ', 'â˜¹': ' negative ', "ğŸ¤¢": "negative", "ğŸ¤§": "negative", 'ğŸ˜´': ' positive ', 'ğŸ’¤': ' positive ',
    "ğŸ˜ˆ": "negative", "ğŸ‘¿": "negative", "ğŸ‘¹": "negative", "ğŸ‘º": "negative", "ğŸ’©": "negative", "ğŸ‘»": "positive",
    "ğŸ’€": "negative", "â˜ ": "negative", "ğŸ‘½": "negative", "ğŸ¤–": "positive", "ğŸƒ": "positive", 'ğŸ˜º': ' positive ',
    'ğŸ˜¸': ' positive ', 'ğŸ˜¹': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ˜¼': ' positive ', 'ğŸ˜½': ' positive ',
    'ğŸ™€': ' negative ', 'ğŸ˜¿': ' negative ', 'ğŸ˜¾': ' negative ', 'ğŸ‘': ' positive ',
    'ğŸ¤²': ' positive ', 'ğŸ™Œ': ' positive ', "ğŸ‘": "positive", "ğŸ™": "positive", 'ğŸ¤': ' positive ', 'ğŸ‘': ' positive ',
    "ğŸ‘": "negative", "ğŸ‘Š": "positive", "âœŠ": "positive", "ğŸ¤›": "positive", "ğŸ¤œ": "positive", "ğŸ¤": "positive",
    "âœŒ": "positive", "ğŸ¤˜": "positive", "ğŸ¤Ÿ": "positive", "ğŸ‘Œ": "positive", "ğŸ‘ˆ": "positive", 'ğŸ‘‰': ' positive ',
    'ğŸ‘†': ' positive ', 'ğŸ‘‡': ' positive ', 'â˜': ' positive ', 'âœ‹': ' positive ', 'ğŸ¤š': ' positive ',
    'ğŸ–': ' positive ', 'ğŸ––': ' positive ', 'ğŸ‘‹': ' positive ', 'ğŸ¤™': ' positive ',
    'ğŸ’ª': ' positive ', 'ğŸ–•': ' negative ', 'âœ': "positive", 'ğŸ¤³': "positive", 'ğŸ’…': ' positive ', 'ğŸ‘„': ' positive ',
    'ğŸ‘…': "positive", 'ğŸ‘‚': "positive", 'ğŸ‘ƒ': "positive", 'ğŸ‘': "positive", 'ğŸ‘€': "positive", 'ğŸ§ ': "positive",
    'ğŸ‘¤': "negative", 'ğŸ‘¥': "negative", 'ğŸ—£': "negative", 'ğŸ‘¶': "positive", 'ğŸ§’': "positive", 'ğŸ‘¦': ' positive ',
    'ğŸ‘§': ' positive ', 'ğŸ§‘': ' positive ', 'ğŸ‘¨': ' negative ', 'ğŸ§”': ' negative ', 'ğŸ‘±â€â™‚ï¸': ' positive ',
    'ğŸ‘©': ' positive ', 'ğŸ‘±â€â™€ï¸': ' positive ', 'ï¸ğŸ§“': ' positive ', 'ğŸ‘´': ' positive ',
    'ğŸ‘µ': ' positive ', 'ğŸ‘²': ' positive ', 'ğŸ‘³â€â™€ï¸': "positive", 'ğŸ‘³â€â™‚ï¸': "positive", 'ğŸ§•': ' positive ', 'ğŸ¤¶': "positive ",
    'ğŸ…': "positive", 'ğŸ‘¼': "positive", 'ğŸ‘¸': "positive", 'ğŸ¤´': "positive", 'ğŸ‘°': "positive", 'ğŸ¤µâ€â™€ï¸': "positive",
    'ğŸ¤µ': "positive", 'ğŸ•´ï¸â€â™€ï¸': "positive", 'ğŸ•´': "positive", 'ğŸ§™â€â™€ï¸': "positive", 'ğŸ§™â€â™‚ï¸': "positive", 'ğŸ§â€â™€ï¸': ' positive ',
    'ğŸ§â€â™‚ï¸': ' positive ', 'ğŸ§šâ€â™€ï¸': ' positive ', 'ğŸ§šâ€â™‚ï¸': ' positive ', 'ğŸ§â€â™€ï¸': ' positive ', 'ğŸ§â€â™‚ï¸': ' positive ',
    'ğŸ§œâ€â™€ï¸': ' positive ', 'ğŸ§œâ€â™‚ï¸': ' positive ', 'ğŸ§›â€â™€ï¸': ' positive ', 'ğŸ§›â€â™‚ï¸': ' positive ',
    'ğŸ§Ÿâ€â™€ï¸': ' positive ', 'ğŸ§Ÿâ€â™‚ï¸': ' positive ', 'ğŸ™‡â€â™€ï¸': "positive", 'ğŸ™‡â€â™‚ï¸': "positive", 'ğŸ’â€â™€ï¸': ' positive ',
    'ğŸ’â€â™‚ï¸': "positive ",  'ğŸ™…â€â™€ï¸': "negative", 'ğŸ™…â€â™‚ï¸': "negative", 'ğŸ™†â€â™€ï¸': "positive", 'ğŸ™†â€â™‚ï¸': "positive",
    'ğŸ¤·â€â™€ï¸': "negative", 'ğŸ¤·â€â™‚ï¸': "negative", 'ğŸ™‹â€â™€ï¸': "positive", 'ğŸ™‹â€â™‚ï¸': "positive", 'ğŸ¤¦â€â™€ï¸': "negative",
    'ğŸ¤¦â€â™‚ï¸': "negative", 'ğŸ™â€â™€ï¸': "positive", 'ğŸ™â€â™‚ï¸': ' positive ',
    'ğŸ™â€â™€ï¸': ' positive ', 'ğŸ™â€â™‚ï¸': ' positive ', 'ğŸ’‡â€â™€ï¸': ' positive ', 'ğŸ’‡â€â™‚ï¸': ' positive ', 'ğŸ’†â€â™€ï¸': ' positive ',
    'ğŸ’†â€â™‚ï¸': ' positive ', 'ğŸ¤°': ' positive ', 'ğŸ¤±': ' positive ', 'ğŸš¶â€â™€ï¸': ' positive ',
    'ğŸš¶â€â™‚ï¸': ' positive ', 'ğŸƒâ€â™€ï¸': ' positive ', 'ğŸƒâ€â™‚ï¸': "positive", 'ğŸ‘«': "positive", 'ğŸ‘¬': ' positive ',
    'ğŸ‘­': "positive ", 'ğŸ’‘': "positive", 'ğŸ‘©â€â¤ï¸â€ğŸ‘©': "positive", 'ğŸ‘¨â€â¤ï¸â€ğŸ‘¨': "positive", 'ğŸ’': "positive",
    'ğŸ‘©â€â¤ï¸â€ğŸ’‹â€ğŸ‘©': "positive", 'ğŸ‘¨â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨': "positive", 'â¤': "positive", 'ğŸ§¡': "positive", 'ğŸ’›': "positive",
    'ğŸ’š': "positive", 'ğŸ’™': "positive", 'ğŸ’œ': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ’”': ' negative ',
    'â£': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’“': ' positive ', 'ğŸ’—': ' positive ',
    'ğŸ’–': ' positive ', 'ğŸ’˜': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’Ÿ': ' positive ', 'ğŸŒ¼': ' positive ',
    "ğŸš«": "negative", 'like': ' positive', 'ğŸ’Œ': ' positive ', ':(': ' negative ', '?': ' ? ',
    'ğŸ’¯': ' positive ', '^^': ' positive ', ':((': ' negative ', 'ï¸ğŸ†—ï¸': ' positive ', ':v': '  positive ',
    '=))': '  positive ', ':3': ' positive ', 'âŒ': ' negative ', ';)': ' positive ','(y)': ' positive',
    '<3': ' positive ', ':))': ' negative ', ':)': ' negative ', ': ) )': ' negative ', ': )': ' negative ',
    '^ ^': 'positive', '^_^': 'positive', ':V': 'positive', ';))': 'positive', ': D': ' positive', ': P': 'positive',
    '= . =': 'negative', '=.=': 'negative', "=='": 'negative', '^ o ^': 'positive', '^o^': ' positive',
    'haizzz': 'negative', 'haiz': 'negative', 'haizz': 'negative', 'kkk': 'positive',
    'he he': ' positive ', 'hehe': ' positive ', 'hihi': ' positive ', 'haha': ' positive ',
    'hjhj': ' positive ', ' lol ': ' positive ', 'huhu': ' negative ', ' 4sao ': ' positive ', ' 5sao ': ' positive ',
    ' 1sao ': ' negative ', ' 2sao ': ' negative ', 'kaka': 'positive', 'ka ka': 'positive', 'ka ka ka': 'positive'

}

wrong_terms = {
    ' acc ': ' taÌ€i_khoaÌ‰n ', ' fb ': ' facebook ', ' ad ': ' admin ', ' ahbp ': ' anh_huÌ€ng_baÌ€n_phiÌm ',
    ' atsm ': ' aÌ‰o_tÆ°Æ¡Ì‰ng_sÆ°Ìc_maÌ£nh ', ' avt ': ' aÌ‰nh_Ä‘aÌ£i_diÃªÌ£n ', ' ava ': ' aÌ‰nh_Ä‘aÌ£i_diÃªÌ£n',' ac ': ' anh_chiÌ£ ',
    ' bb ': ' taÌ£m_biÃªÌ£t ',  ' bla bla ': ' vÃ¢n_vÃ¢n ', ' bsvv ': ' buÃ´Ì‰i_saÌng_vui_veÌ‰ ', ' buÌ€ng ': ' khÃ´ng_traÌ‰_tiÃªÌ€n',
    ' fa ': ' cÃ´_Ä‘Æ¡n ', ' nn ': ' nguÌ‰_ngon ', ' pr ': ' quaÌ‰ng_caÌo ',' bn ': ' bao_nhiÃªu ',
    ' pp ': ' taÌ£m_biÃªÌ£t ',  ' bth ': ' biÌ€nh_thÆ°Æ¡Ì€ng ', ' bt ': ' biÃªÌt ',' cute ': ' dÃªÌƒ_thÆ°Æ¡ng ',
    ' chs ': ' chaÌ‰_hiÃªÌ‰u_sao ', ' cmt ': ' biÌ€nh_luÃ¢Ì£n ', ' ccmnr ': ' chuÃ¢Ì‰n ', ' Ä‘hn ': ' Ä‘eÌo_hiÃªÌ‰u_nÃ´Ìƒi ',
    ' Ä‘hs ': ' Ä‘eÌo_hiÃªÌ‰u_sao ', ' g9 ': ' nguÌ‰_ngon ', ' hpbd ': ' sinh_nhÃ¢Ì£t_vui_veÌ‰ ',
    ' snvv': ' sinh_nhÃ¢Ì£t_vui_veÌ‰ ', ' ib ': ' nhÄƒÌn_tin_riÃªng ', ' kb ': ' kÃªÌt_baÌ£n ', ' sml ': ' sÃ¢Ìp_mÄƒÌ£t_luÃ´n ',
    ' dz ': ' Ä‘eÌ£p_trai ',' dth ': ' dÃªÌƒ_thÆ°Æ¡ng ', ' dt ': ' dÃªÌƒ_thÆ°Æ¡ng ',  ' ex ': ' ngÆ°Æ¡Ì€i_yÃªu_cuÌƒ ',
    ' klq ': ' khÃ´ng_liÃªn_quan ', ' mem ': ' thaÌ€nh_viÃªn ', ' mng ': ' moÌ£i_ngÆ°Æ¡Ì€i ', ' mn ': ' moÌ£i_ngÆ°Æ¡Ì€i ',
    ' nx ': ' nhÃ¢Ì£n_xeÌt ', ' nyc ': ' ngÆ°Æ¡Ì€i_yÃªu_cuÌƒ ', ' omg ': ' oh_my_god ', ' ps ': ' ghi_chuÌ ',
    ' qtqÄ‘ ': ' quaÌ_trÆ¡Ì€i_quaÌ_Ä‘aÌt ', ' rep ': ' traÌ‰_lÆ¡Ì€i ', ' scÄ‘ ': ' sao_cuÌƒng_Ä‘Æ°Æ¡Ì£c ', ' Ã´Ì cÃª ': ' ok ',
    ' stt ': ' traÌ£ng_thaÌi ', ' sub ': ' phuÌ£_Ä‘ÃªÌ€ ', ' tag ': ' gÄƒÌn_theÌ‰ ', ' tÄ‘n ': ' thÃªÌ_Ä‘eÌo_naÌ€o ',
    ' troll ': ' chÆ¡i_khÄƒm ', ' vs ': ' vÆ¡Ìi ', ' ny ': ' ngÆ°Æ¡Ì€i_yÃªu ', ' plz ': ' nÄƒn_niÌ‰ ', ' app ': ' Æ°Ìng_duÌ£ng ',
    ' nt ': ' nhÄƒÌn_tin ', ' trc ': ' trÆ°Æ¡Ìc ',  ' t ': ' tÃ´i ', ' m ': ' miÌ€nh ',
    ' cs ': ' cuÃ´Ì£c_sÃ´Ìng ', ' Ã´Ì kÃª ': ' ok ', ' kp ': ' khÃ´ng_pháº£i ', ' Ã´ cÃª ': ' ok ', ' giÃªÌ€ ': ' giÌ€ ',
    ' zth ': ' dÃªÌƒ_thÆ°Æ¡ng ', ' Ã´ kÃªi ': ' ok ', ' okie ': ' ok ', ' o kÃª ': ' ok ', ' okey ': ' ok ', ' Ã´kÃª ': ' ok ',
    ' oki ': ' ok ', ' ote ':  ' ok ', ' okay ': ' ok ', ' okÃª ': ' ok ', ' oke ': ' ok ', ' Ã´Ì sÃ¬ kÃª ': ' ok ',
    ' khong ': ' khÃ´ng ', ' not ': ' khÃ´ng ', ' kh ': ' khÃ´ng ', ' kÃ´ ': ' khÃ´ng ', ' hok ': ' hÃ´ng ',
    ' ko ': ' khÃ´ng ', ' hk ': ' hÃ´ng ', ' k ': ' khÃ´ng ',  ' guÌt guÌt ': ' tÃ´Ìt ',
    'kg ': ' khÃ´ng ', 'not': ' khÃ´ng ', ' kg ': ' khÃ´ng ', ' "k ': ' khÃ´ng ',
    'kÃ´': ' khÃ´ng ', 'hok': ' khÃ´ng ', '"ko ': ' khÃ´ng ',' mik ': ' mÃ¬nh ', ' mÃ¬n ': ' mÃ¬nh', ' má»nh ': ' mÃ¬nh ',
    'khong': ' khÃ´ng ', ' mk ': ' mÃ¬nh ', ' wÃ¡ ': ' quÃ¡ ', ' qÃ¡ ': ' quÃ¡ ', ' táº¹c vá»i ': ' tuyÃªÌ£t_vÆ¡Ì€i ', ' tiá»‡c dá»i ': ' tuyÃªÌ£t_vÆ¡Ì€i ',
    ' táº¹c zá»i ': ' tuyÃªÌ£t_vÆ¡Ì€i ', ' Ä‘c ': ' Ä‘Æ°á»£c ', ' dc ': ' Ä‘Æ°Æ¡Ì£c ', ' j ': ' gÃ¬ ',
    ' nv ': ' nhÃ¢n_viÃªn ', ' sv ': ' sinh_viÃªn ', ' hs ': ' hoÌ£c_sinh ', ' Ä‘t ': ' Ä‘iÃªÌ£n_thoaÌ£i ', ' ng ': ' ngÆ°Æ¡Ì€i ',
    ' mÃ ng hÃ¬nh ': ' mÃ n_hÃ¬nh ', ' mÃ n hÃ¬n ': 'mÃ n_hÃ¬nh', ' tet ': ' kiá»ƒm_tra ', ' test ': ' kiá»ƒm_tra ',
    ' tÃ©t ': ' kiá»ƒm_tra ', ' sg ': ' saÌ€i_goÌ€n ', ' nvien ': ' nhÃ¢n_viÃªn ', ' siu ': ' siÃªu ', ' paÌ‰i ': ' phaÌ‰i ',
    ' fai ': ' pháº£i ', ' fáº£i ': ' pháº£i ', ' ph ': ' phaÌ‰i ', ' h ': ' giÆ¡Ì€ ', ' sd ': ' sÆ°Ì‰_duÌ£ng ',
    ' of ': ' cuÌ‰a ', ' kon ': ' con ', ' way ': ' quay ', ' s ': ' sao ', ' caÌƒ ': ' caÌ‰ ', ' v ': ' vÃ¢Ì£y ',
    ' r ': ' rÃ´Ì€i ', ' kiu ': ' kÃªu ', ' tl ': ' traÌ‰_lÆ¡Ì€i ', ' thik ': 'thiÌch', ' thiÌc ': 'thiÌch', ' ns ': ' noÌi ',
    ' nviÃªn ': ' nhÃ¢n_viÃªn ', ' nhiu ': ' nhiÃªu ', ' oder ': ' goÌ£i_moÌn ', ' oÌ‰der ': 'goÌ£i_moÌn', ' hiÌ‰u ': ' hiÃªÌ‰u ',
    ' film ': 'phim', ' phin ': ' phim ', ' fim ': ' phim ', ' nh ': ' nhÆ°ng ', ' hnay ': ' hÃ´m_nay ',
    ' nhiÌ€u ': 'nhiÃªÌ€u', ' qay ': ' quay ', ' nc ': ' noÌi_chuyÃªÌ£n ', ' nch ': ' noÌi_chuyÃªÌ£n ', ' tp ': ' thaÌ€nh_phÃ´Ì ',
    ' lun ': ' luÃ´n ', ' ráº¥t tiáº¿t ': ' ráº¥t_tiáº¿c ', ' ráº¥t tiÃªc ': ' ráº¥t_tiáº¿c ', ' toáº¹t zá»i ': ' tuyÃªÌ£t_vÆ¡Ì€i ',
    ' thÃ¢Ìt zoÌ£ng ': ' thÃ¢Ìt_voÌ£ng ', ' thÃ¢Ìt doÌ£ng ': ' thÃ¢Ìt_voÌ£ng ', ' nhÆ°g ': ' nhÆ°ng ', ' ms ': ' mÆ¡Ìi ',
    ' so ciu ': ' dÃªÌƒ_thÆ°Æ¡ng ', ' iu ': ' yÃªu ', ' hn ': ' hÃ´m_nay '
}


def load_list(path):
    with open(path, 'r', encoding='utf-8') as f:
        text = [ViTokenizer.tokenize(t.replace("\n", "")) for t in f.readlines()]
    return text

pos_list = load_list("pos.txt")
neg_list = load_list("neg.txt")
not_list = load_list("not.txt")
neutral_list = load_list("neutral.txt")

s1 = u'Ã€ÃÃ‚ÃƒÃˆÃ‰ÃŠÃŒÃÃ’Ã“Ã”Ã•Ã™ÃšÃÃ Ã¡Ã¢Ã£Ã¨Ã©ÃªÃ¬Ã­Ã²Ã³Ã´ÃµÃ¹ÃºÃ½Ä‚ÄƒÄÄ‘Ä¨Ä©Å¨Å©Æ Æ¡Æ¯Æ°áº áº¡áº¢áº£áº¤áº¥áº¦áº§áº¨áº©áºªáº«áº¬áº­áº®áº¯áº°áº±áº²áº³áº´áºµáº¶áº·áº¸áº¹áººáº»áº¼áº½áº¾áº¿á»€á»á»‚á»ƒá»„á»…á»†á»‡á»ˆá»‰á»Šá»‹á»Œá»á»á»á»á»‘á»’á»“á»”á»•á»–á»—á»˜á»™á»šá»›á»œá»á»á»Ÿá» á»¡á»¢á»£á»¤á»¥á»¦á»§á»¨á»©á»ªá»«á»¬á»­á»®á»¯á»°á»±á»²á»³á»´á»µá»¶á»·á»¸á»¹'
s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'

class VietnameseProcess:
    def __init__(self, sentence):
        self.sentence = sentence

    def tokenize(self):
        sentence = ViTokenizer.tokenize(self.sentence)
        self.sentence = ' ' + sentence + ' '

    def lowercase(self):
        self.sentence = self.sentence.lower()

    def remove_stopwords(self):
        self.sentence = ' ' + self.sentence + ' '
        for w in stopwords_dash:
            #self.sentence = self.sentence.replace("%s " % w, " ")
            self.sentence = self.sentence.replace(" %s " % w, " ")
        for w in sentiment_stopwords:
            self.sentence = self.sentence.replace(" %s " % w, " ")

    def remove_URLs(self):
        self.sentence = re.split('http\S+', self.sentence)
        self.sentence = " ".join(self.sentence)

    def replace_wrong_terms(self):
        self.sentence = ' ' + self.sentence + ' '
        for key, value in wrong_terms.items():
            if self.sentence.find(key) >= 0:
                self.sentence = self.sentence.replace(key, value)

    def remove_repeated_characters(self):
        self.sentence = re.sub(r'([A-Z\s])\1+', lambda m: m.group(1), self.sentence, flags=re.IGNORECASE)

    def remove_punctuation(self):
        """
        Remove all of the special characters in the sentence

        :return: the sentence after removing the special characters
        """
        punctuation = """!"#$%&\'()*+,-./:;<=>?@[\\]^`{|}~"""
        translator = str.maketrans(punctuation, ' ' * len(punctuation))

        self.sentence = self.sentence.translate(translator)

    def replace_emotion(self):
        for key, value in emotion_icons.items():
            if self.sentence.find(key) >= 0:
                self.sentence = self.sentence.replace(key, value)

    def remove_numbers(self):
        self.sentence = re.split('[0-9]+', self.sentence)
        self.sentence = " ".join(self.sentence)

    def replace_not_terms(self):
        text = re.split("\s*[\s,;]\s*", self.sentence)
        for idx in range(len(text)):
            if idx < len(text)-1 and text[idx] in not_list:
                if text[idx+1] in pos_list:
                    text[idx] = "notpositive"
                    text[idx+1] = ""
                if text[idx+1] in neg_list:
                    text[idx] = "notnegative"
                    text[idx+1] = ""
            elif text[idx] not in not_list:
                if text[idx] in pos_list:
                    text.append(" positive ")
                elif text[idx] in neg_list:
                    text.append(" negative ")

        self.sentence = " ".join(text)

    def remove_punctuation2(self):
        text = re.split("[\W+_]", self.sentence)
        for i in range(len(text)):
            if (len(text[i]) > 1 and text[i][:2] != 'ng' and text[i][:2] != 'nh') or len(text[i]) > 2:
                text[i] = " " + text[i]
        self.sentence = "".join(text)

    def split_attached_words(self):
        self.sentence = " ".join(re.findall('[A-Z][^A-Z]*', self.sentence))

    def progress(self):
        self.remove_URLs()
        self.replace_emotion()
        self.remove_punctuation2()
        self.split_attached_words()
        self.lowercase()
        self.tokenize()
        self.remove_numbers()
        self.remove_repeated_characters()
        self.replace_wrong_terms()
        self.replace_not_terms()
        self.remove_stopwords()
